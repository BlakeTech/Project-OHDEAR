#!/usr/bin/python3

#Script written by JasperTecHK, find it on my github! (If you didn't...somehow....)
#TBF/H this entire script is a bodge. The first variant was too. But this is a larger bodge, since this is literally a person who has not looked at the manual and decided to build a web scraper. Then again, it is the same person that decided to build the first version in bash just so a certain someone could save time. So who's the idiot now, ha!
#...Get on with the script, instead of typing your comments, no one is going to see this anyways.
#...Please send help.
#O.H.D.E.A.R. Opensource Heuristics for the Determination of Explicit Artwork Ratios

#Defining all the libraries required.
from multiprocessing import Pool							#Parallel processing
import json													#Parseing the options menu
import requests												#Web requests
import time													#Time function duration
import os													#OS level handling. Might not need it?
import shutil												#Cloning file.
import pandas as pd											#Nonstandard Library. Excel processor
from openpyxl import load_workbook, Workbook				#Nonstandard Library. Pandas augment.
import tkinter												#Nonstandard Library. GUI maker.
import PySimpleGUI as sg									#Nonstandard Library. Tkinter augment.

#Defining all the variables.
url = 'https://danbooru.donmai.us/counts/posts.json?tags='
bustsurl = 'https://paizukan.com/html/'
secend = '+rating:s'
progbarstate = 0

#PySimpleGUI Themes
sg.theme('Purple')

#DEFining all the functions. Eh? Eh?
#... I'll show myself out.
def dirscan ():	#Directory Scan. Used to allow user to locate the file to scrape.
	layout = [[sg.Text('Which list would you like to scour?')],
		[sg.InputText(), sg.FileBrowse(file_types=(""))],	# file_types here uses "" because I don't want to deal with cutting the file extention off, and the "All types", is really more of a ".", i.e. all files with an extention. Also, because I hate adding the extentions. F/Bite me.
		[sg.Button('Confirm'), sg.Button('Cancel')]]
	window = sg.Window('O.H.D.A.M.N.', layout)
	event, values = window.read()
	conf = 0
	loop = 0
	while conf == 0:
		if not values[0] and not event in (None, 'Cancel'):
			sg.popup('No file selected. Due to limitations of GUI, script must die.')
			exit()
		elif event in (None, 'Cancel'):
			sg.popup('User closed script. :(')
			exit()
		elif event == 'Confirm':
			sg.popup('Confirmed!')
			filechosen = values[0]
			conf = 1
	window.close()
	del window
	return filechosen

def setup (filechoice):	#Setup. Might not show up for the end user, depends if the file to scrape is on file.
	with open('Options.json') as f:
		data=json.load(f)
		filtchk=filechoice.split('/')[-1]
		busts, end = "", ""
		if filtchk in data["busts"]:
			busts = data["busts"][filtchk]
		if filtchk in data["ignored"]:
			end = ""
			progloop = 2
		elif filtchk in data["end"]:
			end = data["end"][filtchk]
			progloop = 2
		else:
			layout = [[sg.Text('Would you like to add any end tags to your list?')],
				[sg.InputText()],
				[sg.Button('Add this!'),sg.Button('Nope!')]]
			window = sg.Window('O.H.D.A.M.N.', layout)
			event, values = window.read()
			if event in (None,'Nope!'):
				sg.popup('Understood!')
				data["ignored"][str(filtchk)] = None
				with open('Options.json', 'w') as g:
					g.write(json.dumps(data, sort_keys=True, indent='\t', separators=(',', ': ')))
				end = ""
			elif event == 'Add this!':
				sg.popup('Added to the settings!')
				data["end"][str(filtchk)] = values[0]
				with open('Options.json', 'w') as g:
					g.write(json.dumps(data, sort_keys=True, indent='\t', separators=(',', ': ')))
				end = data["end"][filtchk]
			window.close()
			del window
		return busts, end

def reqParse (filechoice): # Requests url setup here.
	with open(filechoice) as r:
		query = []
		names = []
		for line in r:
			line = line.rstrip("\n")
			link = url + line + end
			query.append(link)
			names.append(line)
	return query, names

def reqParseS (infodmp):	#Appending "s" tag
	query = []
	for line in infodmp:
		link = line + secend
		query.append(link)
	return query

def reqProc (toget): # Requests urls and processes it. Asyncronous Multiprocessing. The bodged way.
	response = requests.get(toget)
	response = response.text
	response = response.replace('{"counts": {"posts"', '').replace('}}', '').replace(':', '')
	return response

def reqProcB(tag):
	if not tag == '':
		layout = [[sg.Text('Would you like to search for bust size too?')],
				  [sg.Button('Hell Yes!'), sg.Button('Nah...')]]
		window = sg.Window('O.H.D.A.M.N.', layout)
		event, garbval = window.read()
		if event in ('Hell Yes!'):
			sg.popup('Please use this responsibly, as to not DOS attack the server!')
			toget = bustsurl + tag
			bust = []
			cup = []
			check1 = 'data-bust="'
			check2 = 'data-cup="'
			response = requests.get(toget)
			response = response.text
			for line in response.splitlines():
				chunk = line.split(' ')
				for i in chunk:
					if check1 in i:
						t1 = i.replace(check1, '').replace('"','')
						cup.append(t1)
					elif check2 in i:
						t1 = i.replace(check2, '').replace('"','')
						bust.append(t1)
		elif event in (None, 'Nah...'):
			sg.popup('Alright then.')
		window.close()
		del window
	else:
		bust = []
		cup = []
	return bust, cup

def dictmerge(d1, d2, d3, d4, d5):
	filename = filechoice.replace('url', 'results') + '.xlsx'
#	wb = load.workbook(filename)
#	ws = wb.get_sheet_by_name('Sheet1')
	wb=Workbook()
	ws=wb.active
	if busts == '':
		for item in zip(d1,d2,d3):
			ws.append(item)
	else:
		for item in zip(d1,d2,d3,d4,d5):
			ws.append(item)
	wb.save(filename)

filechoice = dirscan()
time1 = time.time()
busts, end = setup(filechoice)
query1, list1 = reqParse(filechoice)
asycry = Pool()
list2= asycry.map(reqProc, query1)
list2 = list(list2)
query2 = reqParseS(query1)
list3 = asycry.map(reqProc, query2)
list3 = list(list3)
list4, list5 = reqProcB(busts)
dictmerge(list1, list2, list3, list4, list5)
time1 = time.time() - time1
time1 = round(time1)
print(time1)
