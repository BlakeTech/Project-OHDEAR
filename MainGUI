#!/usr/bin/python3

#Script written by JasperTecHK, find it on my github! (If you didn't...somehow....)
#TBF/H this entire script is a bodge. The first variant was too. But this is a larger bodge, since this is literally a person who has not looked at the manual and decided to build a web scraper. Then again, it is the same person that decided to build the first version in bash just so a certain someone could save time. So who's the idiot now, ha!
#...Get on with the script, instead of typing your comments, no one is going to see this anyways.
#...Please send help.
#O.H.D.E.A.R. Opensource Heuristics for the Determination of Explicit Artwork Ratios

#Defining all the libraries required.
from multiprocessing import Pool	#Parallel processing
import json							#Parseing the options menu
import requests						#Web requests
import time							#Time function duration
import os							#OS level handling. Might not need it?
import re							#Regex Library. Might not need it?
import shutil						#Cloning file.
import pandas as pd					#Nonstandard Library. Excel processor
import openpyxl						#NOnstandard Library. Pandas augment.
import tkinter						#Nonstandard Library. GUI maker.
import PySimpleGUI as sg			#Nonstandard Library. Tkinter augment.

#Defining all the variables.
url = 'https://danbooru.donmai.us/counts/posts.json?tags='
bustsurl = 'https://paizukan.com/html/'
secend = '+rating:s'
progbarstate = 0

#PySimpleGUI Themes
sg.theme('Purple')

#DEFining all the functions. Eh? Eh?
#... I'll show myself out.
def dirscan ():	#Directory Scan. Used to allow user to locate the file to scrape.
	layout = [[sg.Text('Which list would you like to scour?')],
		[sg.InputText(), sg.FileBrowse(file_types=(""))],	# file_types here uses "" because I don't want to deal with cutting the file extention off, and the "All types", is really more of a ".", i.e. all files with an extention. Also, because I hate adding the extentions. F/Bite me.
		[sg.Button('Confirm'), sg.Button('Cancel')]]
	window = sg.Window('O.H.D.A.M.N.', layout)
	event, values = window.read()
	conf = 0
	loop = 0
	while conf == 0:
		if not values[0] and not event in (None, 'Cancel'):
			sg.popup('No file selected. Due to limitations of GUI, script must die.')
			exit()
		elif event in (None, 'Cancel'):
			sg.popup('User closed script. :(')
			exit()
		elif event == 'Confirm':
			sg.popup('Confirmed!')
			filechoice = values[0]
			conf = 1
	window.close()
	del window
	return filechoice

def setup (filechoice):	#Setup. Might not show up for the end user, depends if the file to scrape is on file.
	with open('Options.json') as f:
		data=json.load(f)
		filtchk=filechoice.split('/')[-1]
		busts, end = "", ""
		if filtchk in data["ignored"]:
			end = ""
			progloop = 2
		elif filtchk in data["end"]:
			end = data["end"][filtchk]
			progloop = 2
		else:
			layout = [[sg.Text('Would you like to add any end tags to your list?')],
				[sg.InputText()],
				[sg.Button('Add this!'),sg.Button('Nope!')]]
			window = sg.Window('O.H.D.A.M.N.', layout)
			event, values = window.read()
			if event in (None,'Nope!'):
				sg.popup('Understood!')
				data["ignored"][str(filtchk)] = None
				with open('Options.json', 'w') as g:
					g.write(json.dumps(data, sort_keys=True, indent='\t', separators=(',', ': ')))
				end = ""
			elif event == 'Add this!':
				sg.popup('Added to the settings!')
				data["end"][str(filtchk)] = values[0]
				with open('Options.json', 'w') as g:
					g.write(json.dumps(data, sort_keys=True, indent='\t', separators=(',', ': ')))
				end = data["end"][filtchk]
			if filtchk in data["busts"]:
				busts = data["busts"][filtchk]
			window.close()
			del window
		return busts, end

def reqParse (filechoice): # Requests url setup here.
	with open(filechoice) as r:
		query = []
		for line in r:
			line = line.rstrip("\n")
			link = url + line + end
			query.append(link)
	return query

def reqParseS (infodmp):	#Appending "s" tag
	query = []
	for line in infodmp:
		link = line + secend
		query.append(link)
	return query

def reqProc (toget): # Requests urls and processes it. Asyncronous Multiprocessing. The bodged way.
	response = requests.get(toget)
	response = response.text
	response = response.replace('{"counts": {"posts"', '').replace('}}', '')
	name=toget.replace(url, '')				#Also gets rid of the url.
	name=name.replace(secend, '')
	if not end == '':
		name=name.replace(end, '')
	if 'hold' in globals():
		response = response.replace(': ', ',')
	else:
		response = response.replace(': ', '')
	return name,response

def dictmerge(d1, d2):
	filename = filechoice.replace('url', 'results') + '.xlsx'
	print(filename)
	print(d1)
	print(d2)


#	with open(filename, 'w+') as f:
#		w = csv.writer(f)
#		w.writerows(d1.items())
#		for i in dict.item():
#			item = str(i)
#			push = re.sub('[^a-zA-Z0-9\n\,]', '', item)
#			push = item
#			w.writerow(push)

filechoice = dirscan()
time1 = time.time()
busts, end = setup(filechoice)
query1 = reqParse(filechoice)
asycry = Pool()
hold1 = asycry.map(reqProc, query1)
hold1 = dict(hold1)
query2 = reqParseS(query1)
hold2 = asycry.map(reqProc, query2)
hold2 = dict(hold2)
dictmerge(hold1,hold2)
#if not busts == '':
#	reqProcB()


#if os.name == 'nt':
#	name = filechoice.rsplit('\', 1)
#elif os.name == 'posix':
#	name == filechoice.rsplit('/, 1)
#purgetofile(filechoice, dictmerged)
time1 = time.time() - time1
time1 = round(time1)
print(time1)
#print(dictmerged)
#for k, v in hold.items():
#    print(f"{k},{v}")
#progstat = 1
#print(hold1)
#print("\n\n")
#print(hold2)
#del window
