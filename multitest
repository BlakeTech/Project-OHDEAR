#!/usr/bin/python3

import multiprocessing
import json
import requests
import asyncio
import aiohttp
import threading
from queue import Queue
import sys

url = 'https://danbooru.donmai.us/counts/posts.json?tags='
end = '_(kantai_collection)'

def multipooltest(urls):
	result = []
	response = requests.get(urls)
	result.append(response.text)
	return result

async def asyncget(sem, session, link):
	async with sem:
		async with session.get(link) as resp:
#fault lies in here, the get request only does the things below, i.e. returns the variable, if it returns a value.
#It also successfully merges the resulting number back to it's source. I.e. I can match I/O to verify links.
#Thus, if the url doesn't return a value, then whatever variables don't make it through means the url failed to return.
#This has two possible implications. The first, that session is faulty.
#The second is that there is a rate limit, despite claims otherwise.
			i = link.replace(url, "").replace(end, "")
			repy = await resp.text()
			repy = repy.replace("{\"counts\": {\"posts\":", "").replace("}}", "")
			j = resp.status
			print("\t\t" + "\"" + i + "\"" + ": \"" + repy + " " + str(j) + "\",")
			return await resp.text()

async def aioreq(urls):
	sem = asyncio.Semaphore(4)
	async with aiohttp.ClientSession() as session:
		response = await asyncio.gather(
			*[asyncget(sem, session, i) for i in urls]
		)
		return response

def threadtest(q, result):
	while not q.empty():
		work = q.get()
		response = requests.get(work[1])
		result[work[0]] = response.text
		q.task_done()
	return True

def urlsget():
	with open('Kantai Collection', 'r') as r:
		query = []
		name = []
		for line in r:
			line = line.rstrip("\n")
			link = url + line + end
			query.append(link)
			name.append(line)
	return query, name

choice = []
choice = sys.argv
choice = int(choice[1])
urls, name = urlsget()
q = Queue(maxsize=0)
numthreads = min(50, len(urls))

if choice == 1:	#Multiprocessing, Pool
	pool = multiprocessing.Pool()
	results = pool.map(multipooltest, urls)
	pool.close()
	pool.join()
	print(results)

elif choice == 2: #async - not feasible, it keeps timing out for some reason, and is way slower. No idea why.
	try:
		print("{")
		print("\t" "\"track\":{")
		results = asyncio.run(aioreq(urls))
		print(results)
		print(len(results))
	except asyncio.exceptions.TimeoutError:
		print("\t", "}")
		print("}")

elif choice == 4:
	with open ("test", "r") as r:
		data=json.load(r)
		for i in name:
			if not i in data["track"]:
				print(i)
#Deals with the printed json, slight cleanup required. Specifically, last comma to be removed, and odd spacings too. ONly for debugging.

elif choice == 3: #thread
	results = [{} for x in urls]
	threads = []
	for i in range(len(urls)):
		q.put((i,urls[i]))

	for i in range(numthreads):
		process = threading.Thread(target=threadtest, args=(q, results))
		process.setDaemon(True)
		process.start()

	q.join()
	print(results)
	print(len(results))
else:
	print('No result.')
